<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Sampling Distributions</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer-yellow.css" type="text/css" />
    <link rel="stylesheet" href="css/additionalCols.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Sampling Distributions

---








# Recap

Normal distributions all have well-characterized properties
 - AUC = 1
 - ~68% fall within 1 `\(\sigma\)`, ~95% within 2 `\(\sigma\)`, and ~99.7% within 3 `\(\sigma\)`

--

The standard normal distribution is a particular type of normal distribution
 - distribution of `\(z\)`-scores
 - `\(\mu = 0\)`
 - `\(\sigma = 1\)`

--

Using the standard normal &amp; these cool properties, we can make probability statements
  - What is the probability of getting a z-score or more extreme?

---
name: pvs
# What do we want?

We want to make inferences about a population

  - But the population is too large to measure directly
  - So we need to estimate the population parameters
  
---

# Population

- The population distribution is a *theoretical probability distribution* that has some mathematical form

- Ultimately we want to use a sample distribution to understand the population distribution

---

# What is the *point* of inferential stats?

Point estimation: we use our sample statistics to take our best guess of the population parameter

We know that our estimates will vary from sample to sample

We're using our sample as an estimate
  - Sample mean `\(\bar{X}\)` is an **_estimator_** of `\(\mu\)`
  - A specific sample is an **_estiamte_**

---

# Population vs. Sample

|             | Population&lt;br&gt;Distribution |  Sample&lt;br&gt;Distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|
| Distribution consists of:    |    Individual observations&lt;br&gt; `\(x\)`    | Individual observations&lt;br&gt; `\(x\)`       | 
| Central tendency |    `\(\mu\)`   | `\(\bar{x}\)`      | 
| Dispersion | `\(\sigma^2\)` | `\(s^2\)` |
|            | `\(\sigma\)` | `\(s\)` |
| Type       | Parameter | Statistic |
| T vs. O    | Theoretical | Observed |


---
name: sampling
# Sampling Distribution

- The major goal that we have in statistical inference is to make confident claims about the *population* based on a small representation of it, the *sample*.

- Any sample will be off the mark in how well it captures the important features of a population. The **sampling distribution** tells us how far off the mark we can expect a sample statistic to be. 


---
# Sampling Distribution
We use features of the sample (*statistics*) to tell us about features of the population (*parameters*).

The quality of this information goes up as sample size goes up

All sample statistics are wrong, but they become more useful as sample size increases. 

---
# Sampling Distribution

.pull-left[

The parameters of this distribution are unknown.

What can we do? We can use the sample to inform us about the likely characteristics of the population.

]

.pull-right[

&lt;img src="22-slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;
]
---

.left-column[

### Samples from the population 

.small[
Each *sample distribution* will resemble the population. That resemblance will be better as sample size increases.

Statistics (e.g., mean) can be calculated for any sample.
]
]

&lt;img src="22-slides_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

???
here, the sample sizes is 30 fo each of these 4 graphs. each of theses is a sample. and each sample has it's only mean and sd. 
---

.left-column[
.small[

Say you repeated an experiment 100 times, each time using a new sample. Each sample has it's own mean (and other statistics). 

That's 100 means. You can have a distribution of means rather than of scores. That's a sampling distribution! **Distribution of statistics**

The mean of the **sampling distribution** converges on population mean, `\(\mu\)`

]
]

&lt;img src="22-slides_files/figure-html/sampling1-1.png" style="display: block; margin: auto;" /&gt;

---

.left-column[
### Sampling Distribution 
.small[
This distribution has a standard deviation that tells us how typical or rare values of the sample statistic are likely to be.

The sampling distribution of the mean is of particular interest, it's called the **standard error of the mean** (SEM). 
]
]


&lt;img src="22-slides_files/figure-html/sampling2-1.png" style="display: block; margin: auto;" /&gt;

???

Sampling distributions can be constructed around any statistic -- ranges, standard deviations, difference scores. The standard errors of those distributions are also standard errors. (E.g., the standard error of the difference.)

---

# Notation


|             | Population&lt;br&gt;Distribution |  Sample&lt;br&gt;Distribution | Sampling&lt;br&gt;Distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Distribution consists of:    |    Individual observations&lt;br&gt; `\(x\)`    | Individual observations&lt;br&gt; `\(x\)`   | Statistics&lt;br&gt; `\(\bar{x}, s, s^2\)` | 
| Central tendency |    `\(\mu\)`   | `\(\bar{x}\)`      | `\(\mu_M\)` |
| Dispersion | `\(\sigma^2\)` | `\(s^2\)` | `\(\sigma^2_M\)` |
|            | `\(\sigma\)` | `\(s\)` | SEM `\(\sigma_M\)` |
| Type       | Parameter | Statistic | Statistic of statistics |
| T vs. O    | Theoretical | Observed | Theoretical

---

# Sampling Distributions

- Distribution of values of a particular statistic ( `\(\bar{x}\)`, `\(s^2\)`, `\(s\)`) across all possible samples of N observations
- One of the most imporat
---
# Interactive Example

PLAY WITH THIS!

[Sampling distribution example](http://shiny.calpoly.sh/Sampling_Distribution/)

---
# Sampling Distribution Approximates the Normal

One of the most important discoveries in statistics is that the sampling distributions of many statistics are approximately **normal** even when the sample (and population) distributions are not.

The mean of a random sample will not precisely equal the population mean. But, how far off will it be? 

The error represented by how far off a sample mean is from the population mean is called **sampling error**.  

---
name: clt
# Central Limit Theorem

According to the **central limit theorem**, as sample size increases, the sampling distribution of the mean approaches normality, even when the data upon which the mean is based are not normally distributed.

The sample size necessary to be "approximately normal" depends on the nature of the underlying data.  The less normal it is, the larger the sample size necessary in order for the sampling distribution of the means to become normal.

"Around sample size of 30" is a common rule of thumb.

???

Regardless of the CLT, if the data are skewed, we might wonder if the mean is the best estimator to use here.

---
# Central Limit Theorem
.left-column[Ends up that quite a few sample statistics approach normality as sample size increases.

Here is the sample standard deviation from a normal distribution with `\(\sigma = 1\)`.
]

&lt;img src="22-slides_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---
# Central Limit Theorem

.left-column[And the range.]

&lt;img src="22-slides_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
???
Note here that we're not just narrowing in, but our mean estimate is getting larger. 

Remember bias?

---

# Relationship Between Population &amp; Sampling

- If the population is normally distributed, the sampling distribution of the mean will be normally disributed

- If the population distribution is not normally distributed, the sampling distribution of the mean will become increasingly normally disributed as sample size increases

- We can use the normal distribution to make inferences about the unknown population mean, based on the sample mean and sample standard deviation

---
name: sem

# Mean of the Sampling Distribution

The mean of the sampling distribution converges on the population mean, `\(\mu\)`

Our sample mean is not biased; we'll be a little wrong, but it'll be OK

---
# Variability of the Sampling Distribution

**Standard Error of the Mean**
  - The standard deviation of the sampling distribution
  - Directly related to the variability of the underlying data:

`$$\sigma_{m} = \frac{\sigma_x}{\sqrt{N}}$$`

- The smaller the SEM, the more likely it is that your sample estimate of the mean will be closer to the population estimate of the mean
- SEM is a function of sample size! The more accurate your sample mean, the closer you are to approximating the population, and the smaller the standard error 

???
We do not know `\(\sigma\)` but we can estimate it based on the sample statistic:

`$$\hat{\sigma} = s = \sqrt{\frac{\sum(x - \bar{x})^2}{N-1}}$$`

---

# More SEM

`$$\hat{\sigma} = s = \sqrt{\frac{\sum(x - \bar{x})^2}{N-1}}$$`

This is the sample estimate of the population standard deviation.  This is an unbiased estimate of `\(\sigma\)` and relies on the sample mean, which is an unbiased estimate of `\(\mu\)`.

This is different from the sample standard deviation, which divides the sum of squares by `\(N\)` rather than `\(N-1\)`.

`$$SEM = \sigma_M = \frac{\hat{\sigma}}{\sqrt{N}} = \frac{\text{Estimate of pop SD}}{\sqrt{N}}$$`


---
# Making Statements

The sampling distribution of means can be used to make probabilistic statements about means in the same way that the standard normal distribution is used to make probabilistic statements about scores.

For example, we can determine the range within which the population mean is likely to be with a particular level of confidence.

Or, we can propose different values for the population mean and ask how typical or rare the sample mean would be if that population value were true.  We can then compare the plausibility of different such “models” of the population.

???
The key is that we have a sampling distribution of the mean with a standard deviation **(the SEM)** that is linked to the population:

---
name: ci
# Confidence Intervals

The sampling distribution of the mean has variability, represented by the SEM, reflecting uncertainty in the sample mean as an estimate of the population mean.

The assumption of normality allows us to construct an interval within which we have good reason to believe a population mean will fall: 

$$\bar{X} - (1.96\times SEM) \leq \mu \leq \bar{X} + (1.96\times SEM) $$

???
Sampling distributions are normally distributed
---
# Confidence Intervals

$$\bar{X} - (1.96\times SEM) \leq \mu \leq \bar{X} + (1.96\times SEM) $$

- This is referred to as the **95% confidence interval (CI)**
- The 95% CI is sometimes represented as:

`$$CI_{95} = \bar{X} \pm [1.96\frac{\hat{\sigma}}{\sqrt{N}}]$$`

---
name: tdist

.left-column[
.small[
### The `\(t\)`

The normal distribution assumes we know the population mean and standard deviation. But we don’t. We only know the *sample* mean and standard deviation, and those have some uncertainty about them. 

That uncertainty is reduced with large samples, so that it's “close enough” to the normal. In small samples, the `\(t\)` distribution is better.
]
]

&lt;img src="22-slides_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---
# `\(t\)` distribution

- The primary difference between the normal distribution and the `\(t\)` distribution is the fatter tails
  - This produces wider confidence intervals
  - The penalty we have to pay for our ignorance about the population

- The form of the confidence interval remains the same. We simply substitute a corresponding value from the `\(t\)` distribution (using df = `\(N -1\)`).


`$$CI_{95} = \bar{X} \pm [1.96\frac{\hat{\sigma}}{\sqrt{N}}]$$`

`$$CI_{95} = \bar{X} \pm [t_{.975, df = N-1}\frac{\hat{\sigma}}{\sqrt{N}}]$$`

---
# Confidence Intervals

What does it NOT mean?
  - There is a 95% probability that the true mean lies inside the confidence interval

--

What it *actually* means:
  - If we carried out random sampling from the population a large number of times...
  - and calculated the 95% confidence interval each time...
  - then 95% of those intervals can be expected to contain the population mean.


---

.left-column[
.small[
###Simulation

At each sample size, draw 5000 samples from known population ( `\(\mu = 0\)` , `\(\sigma = 1\)` ). 

Calculate CI for each sample using `\(s\)` and record whether or not 0 was in that interval.

Calculate CI using for each sample using `\(\sigma\)`.

]
]

&lt;img src="22-slides_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---
name: ex
## Examples
.small[
In the past, my classroom exams (aggregating over many classes) have a mean of 90 and a standard deviation of 8.

My next class will have 100 students. What range of exam means would be plausible if this class is similar to past classes (comes from the same population)?
]


```r
M = 90
SD = 8
N = 100

sem = SD/sqrt(N)

ci_lb_z = M - sem * qnorm(p = .975)
ci_ub_z = M + sem * qnorm(p = .975)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 88.43203 91.56797
```

```r
ci_lb_z = M - sem * qt(p = .975, df = N-1)
ci_ub_z = M + sem * qt(p = .975, df = N-1)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 88.41263 91.58737
```

---
## Examples
.small[
I give a classroom exam that produces a mean of 83.4 and a standard deviation of 10.6. A total of 26 students took the exam.

What is the 95% confidence interval around the mean?
]

```r
M = 83.4
SD = 10.6
N = 26

sem = SD/sqrt(N)

ci_lb_z = M - sem * qnorm(p = .975)
ci_ub_z = M + sem * qnorm(p = .975)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 79.32557 87.47443
```

```r
ci_lb_z = M - sem * qt(p = .975, df = N-1)
ci_ub_z = M + sem * qt(p = .975, df = N-1)
print(c(ci_lb_z, ci_ub_z))
```

```
## [1] 79.11857 87.68143
```

---
# All Together

|             | Population&lt;br&gt;Distribution |  Sample&lt;br&gt;Distribution | Sampling&lt;br&gt;Distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Distribution consists of:    |    Individual observations&lt;br&gt; `\(x\)`    | Individual observations&lt;br&gt; `\(x\)`   | Statistics&lt;br&gt; `\(\bar{x}, s, s^2\)` | 
| Central tendency |    `\(\mu\)`   | `\(\bar{x}\)`      | `\(\mu_M\)` |
| Dispersion | `\(\sigma^2\)` | `\(s^2\)` | `\(\sigma^2_M\)` |
|            | `\(\sigma\)` | `\(s\)` | SEM `\(\sigma_M\)` |
| Type       | Parameter | Statistic | Statistic of statistics |
| T vs. O    | Theoretical | Observed | Theoretical

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
