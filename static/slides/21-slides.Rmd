---
title: "Normal & Sampling Distributions"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer-orange.css", "css/additionalCols.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation:
        scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = F,
                      message = F,
                      fit.retina = 3,
                      fig.align = "center")

hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#f0932b",
                  outfile = "xaringan-themer-orange")
```

```{r packagesAndData, include=FALSE, warning=FALSE}
library(tidyverse)
library(ggpubr)
library(knitr)
colors = RColorBrewer::brewer.pal(4, "Set2")
```

# Recap
Measures of Central Tendency
  - Mean (average)
  - Median (middle)
  - Mode (most)

--

Measures of Dispersion
  - Variance
  - Standard deviation

--

Standardized Scores

---
name: norm

# The Normal Distribution

The **normal distribution**
  - aka "bell curve" or "Gaussian distribution" 
  - Two-parameter distribution defined by the mean (
$\mu$
) and standard deviation (
$\sigma$
) 

---
# The Normal Distribution

.pull-left[

The **probability density function** gives the height of the curve at a particular value for X. 

Although these values communicate information about probability or likelihood, they are not probabilities.
]

.pull-right[
```{r normal, echo = FALSE, fig.height = 6}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("The normal curve") +
  theme(text = element_text(size = 20))
```

```{r ref.label = "normal", eval=F, echo=FALSE}

```
]

---
# Probability Density Functions

You can take an entire semester long course on probability. Getting into the details is beyond the scope of this class, sadly. What you should know:

- The total area under the curve of a proability density function is **1**
- For a given continuous random variable, the probability of getting any single value is basically **0**


---

# Probability of a single value
.pull-left[
The area under the curve that lies between the mean (here 0) and a value of 1 is the probability of a score between 0 and 1.
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) + 
  stat_function(fun = function(x) dnorm(x) , 
                xlim = c(0, 1),
                geom = "area", fill = "#f0932b") + 
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Variable X") + scale_y_continuous("Density")+ggtitle("The normal curve") + theme(text = element_text(size =20))
```
]

---
# Probability of a single value or more extreme

.pull-left[
We can also make statements about probability like *"the probability of obtaining an x value of 1 __or larger__ is \_____"*
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) + 
  stat_function(fun = function(x) dnorm(x) , 
                xlim = c(1, 4),
                geom = "area", fill = "#f0932b") + 
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Variable X") + scale_y_continuous("Density")+ggtitle("The normal curve") + theme(text = element_text(size =20))
```
]

---

# Characteristics of the normal distribution

* The mean and standard deviation are independent

* The distribution is unimodal and symmetrical

* For two normal distributions, the area under the curve between corresponding locations in standard deviation units is the same regardless of $\mu$ and $\sigma$ 

---
# Family of Normal Distributions

All of these distributions are normal and have an equivalent area (proportion) that falls between a standard deviation below and above their respective means.

```{r, echo = F, fig.width=10, fig.height = 5}
p1 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x) , 
                xlim = c(-1, 1),
                geom = "area", fill = "#f0932b") +
  stat_function(fun = function(x) dnorm(x)) +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("Area under the normal curve")

p2 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, sd = 1.5) , 
                xlim = c(-1.5, 1.5),
                geom = "area", fill = "#f0932b") +
  stat_function(fun = function(x) dnorm(x, sd = 1.5)) +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("Area under the normal curve")

p3 = ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
stat_function(fun = function(x) dnorm(x, sd = .5) , 
                xlim = c(-.5, .5),
                geom = "area", fill = "#f0932b") +
  stat_function(fun = function(x) dnorm(x, sd = .5)) +
  geom_hline(aes(yintercept = 0)) +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density")+
  ggtitle("Area under the normal curve")

ggarrange(p1, p2, p3, ncol = 3)
```

---
# Characteristics of the normal distribution

- About 68.3% of the data will be within one standard deviation of the mean. 
- About 95% of the data will be within two standard deviations of the mean.  
- About 99.7% of the data will be within three standard deviations of the mean.

In other words, nearly **all** of the data will fall within 3 standard deviations of the mean in a normal distribution.

---
name: stdnorm

# Standard normal distribution

A normal distribution with $\mu$=0 and $\sigma$=1 is called **standard normal**. It is one specific distribution that comes from the larger family of normal distributions.  

Variables with quite different means and standard deviations can be standardized so that they can be compared in the same metric (standard deviation units). This allows statements such as "relative to the mean, I am more conscientious (e.g., $z=2$) than I am extraverted (e.g., $z=1$)."

--

All continuous distributions can be standardized, but if they are not normal to begin with, standardization will not make them so.  *Standardization does not alter distribution shape.*

---
# Standardized scores ( $z$-scores)

> Distance from the mean in standard deviation units


$$ z = \frac{x_i - \bar{x}}{s} $$

Properties of $z$-scores:

- $\Large \mu_z = 0$
- $\Large \sigma_z = 1$

--

- Compare across scales and units of measures
- More easily identify extreme data

---
name: use

# Using $z$-scores

.pull-left[
```{r echo=FALSE}
starwars[1:6,1:2]
```
]

--

.pull-right[
```{r}
starwars %>% 
  select(1:2) %>% 
  mutate_at(2, ~round(x = scale(.), digits = 2)) %>% 
  head(.) %>% 
  print(.)
```

]

---

# Using $z$-scores
Given any score, we can calculate the probability of getting a value greater than that $z$-score. *(Or less than that z-score.)*

You can look up tables that give you the probability value that corresponds to any given $z$-score. Or, you can use `R` code. 

--

Luke Skywalker's height is $z$ = -.07

```{r}
pnorm(q = -.07, mean = 0, sd = 1)
```

---
# Using $z$-scores

.pull-left[
The probability of getting a $z$-score of -.07 or greater?

```{r}
1-pnorm(q = -.07, mean = 0, sd = 1)
```
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) + 
  stat_function(fun = function(x) dnorm(x) , 
                xlim = c(-0.07, 4),
                geom = "area", fill = "#f0932b") + 
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Variable X") + scale_y_continuous("Density")+ggtitle("The normal curve") + theme(text = element_text(size =20))
```
]
---
# Using $z$-scores

.pull-left[
What about R2D2? ( $z$-score of -2.25)
  - Probability of getting a $z$-score of -2.25 or something even smaller
  
```{r}
pnorm(q = -2.25, mean = 0, sd = 1)
```
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) + 
  stat_function(fun = function(x) dnorm(x) , 
                xlim = c(-2.25, -4),
                geom = "area", fill = "#f0932b") + 
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Variable X") + scale_y_continuous("Density")+ggtitle("The normal curve") + theme(text = element_text(size =20))
```
]
---
# Using $z$-scores

.pull-left[
Probability of getting a $z$-score of -2.25 or something larger
  
```{r}
1-pnorm(q = -2.25, mean = 0, sd = 1)
```
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x)) + 
  stat_function(fun = function(x) dnorm(x) , 
                xlim = c(-2.25, 4),
                geom = "area", fill = "#f0932b") + 
  geom_hline(aes(yintercept = 0))+
  scale_x_continuous("Variable X") + scale_y_continuous("Density")+ggtitle("The normal curve") + theme(text = element_text(size =20))
```
]

---
# Some $z$-scores of note

- $z = 1.64$; most extreme 5% of the standard normal distribution (the very far tail)

- $z = 1.96$; most extreme 2.5% of the standard normal distribution (used when splitting the difference of most positive and most negative extremes)

---
name: sampling
class: inverse

# Sampling Distributions

---

# What's the point of inferential stats?

.pull-left-small[
- Most of the time, we can't measure an entire population.
- We instead take random samples from the population, and we *estimate* statistics. We treat these as our best guess of the population parameter.
- We know that our statistics will vary from sample to sample.
]

.pull-right-large[
|             | Population<br>Distribution |  Sample<br>Distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|
| Distribution consists of:    |    Individual observations<br> $x$    | Individual observations<br> $x$       | 
| Central tendency |    $\mu$   | $\bar{x}$      | 
| Dispersion | $\sigma^2$ | $s^2$ |
|            | $\sigma$ | $s$ |
| Type       | Parameter | Statistic |
| T vs. O    | Theoretical | Observed |
]

---

### All sample statistics are wrong, but they become more useful as the sample size increases

.pull-left-small[
- The parameters of this population distribution are unknown

- We use the sample to inform us about the likely characteristics of the population
]

.pull-right-large[
```{r, echo = F, warning = F, message = F}
ggplot(data.frame(x = seq(-4, 4)), aes(x)) +
  stat_function(fun = function(x) dnorm(x), geom = "area", fill = "#f0932b") +
  scale_x_continuous("Variable X") +
  scale_y_continuous("Density") +
  theme_classic() +
  theme(text = element_text(size = 20)) 

```
]

---
### Samples from the population 

.pull-left-small[
Each *sample distribution* will resemble the population. That resemblance will be better as sample size increases.

Statistics (e.g., mean) can be calculated for any sample.
]

.pull-right-large[
```{r, echo=FALSE, warning=F, message=F}
library(ggpubr) #for multiple plots
sample_size = 30
set.seed(101919)
for(i in 1:4){
  sample = rnorm(n = sample_size)
  m = round(mean(sample),3)
  s = round(sd(sample),2)
  p = data.frame(x = sample) %>%
    ggplot(aes(x = x)) +
    geom_histogram(color = "white") +
    geom_vline(aes(xintercept = m), color = "#f0932b", size = 2, alpha = .5)+
    scale_x_continuous(limits = c(-4,4)) +
    scale_y_continuous("", breaks = NULL) +
    labs(title = as.expression(bquote("Sample"~.(i)~", m ="~.(m)~", sd ="~.(s)))) +
    theme_classic()
  assign(paste0("p",i), p)
}

ggarrange(p1,p2,p3,p4, ncol =2, nrow = 2)
```
]

---
### Sampling Distribution

.pull-left-small[
- Say you repeated an experiment 100 times, each time using a new sample. Nothing else changes. Each sample has it's own mean (and other statistics).
- That's 100 means. You can have a distribution of means rather than of scores. That's a sampling distribution! **Distribution of statistics**
]

.pull-right-large[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
reps = 5000
means = rep(0, reps)
se = 1/sqrt(sample_size)
set.seed(101919)
for(i in 1:reps){
  means[i] = mean(rnorm(n = sample_size))
}
data.frame(mean = means) %>%
  ggplot(aes(x = mean)) + 
  geom_histogram(aes(y = ..density..), 
                 fill = "#f0932b", 
                 color = "white") +  
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = se), inherit.aes = F, size = 1.5) +
  labs(title = "Sampling Distribution")
```

]

---
### Sampling Distribution

.pull-left-small[
- The mean of the sampling distribution converges on the population mean, $\mu$ 
- The sampling distribution can also have it's own spread (variance/standard deviation). This tell us how typical or rare the sample statistic is likely to be. We call this the **standard error of the mean** (SEM). 
- Note it could be any statistic (standard error of the median, standard error of the range etc.)
]

.pull-right-large[
```{r, echo=FALSE, message=FALSE, warning=FALSE}
reps = 5000
means = rep(0, reps)
se = 1/sqrt(sample_size)
set.seed(101919)
for(i in 1:reps){
  means[i] = mean(rnorm(n = sample_size))
}
data.frame(mean = means) %>%
  ggplot(aes(x = mean)) + 
  geom_histogram(aes(y = ..density..), 
                 fill = "#f0932b", 
                 color = "white") +  
  stat_function(fun = function(x) dnorm(x, mean = 0, sd = se), inherit.aes = F, size = 1.5) +
  labs(title = "Sampling Distribution")
```

]

---

# Notation


|             | Population<br>Distribution |  Sample<br>Distribution | Sampling<br>Distribution |
|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Distribution consists of:    |    Individual observations<br> $x$    | Individual observations<br> $x$   | Statistics<br> $\bar{x}, s, s^2$ | 
| Central tendency |    $\mu$   | $\bar{x}$      | $\mu_M$ |
| Dispersion | $\sigma^2$ | $s^2$ | $\sigma^2_M$ |
|            | $\sigma$ | $s$ | SEM $\sigma_M$ |
| Type       | Parameter | Statistic | Statistic of statistics |
| T vs. O    | Theoretical | Observed | Theoretical

---

# Sampling Distributions

- Distribution of values of a particular statistic ( $\bar{x}$, $s^2$, $s$) across all possible samples of N observations
- One of the most important discoveries in statistics is that the sampling distributions of many statistics are approximately **normal** even when the sample (and population) distributions are not normal!
- [Play around with this if you want to prove it to yourself](http://shiny.calpoly.sh/Sampling_Distribution/)
- Why does this matter? Because as we saw earlier, the normal distribution is awesome!

---

# Scroll back

- Remember that whole exercise we did earlier in this lecture with the Starwars dataset?

  - We took a vector of heights
  - Turned them into $z$-scores
  - Asked *"how likely is it that we got this particular $z$-score or something more extreme?"*

--

- Now, we are going to do this exact sampe procedure, but this time, rather than working with individual scores, we're going to work with **means**. Is it likely or unlikely that we got a particular mean (or more extreme), if it comes from a particular sampling distribution of means? 

--

_**This is what we're actually interested in!**_

---
class: inverse

# Next time...

- Comparing means with NHST...putting it all together